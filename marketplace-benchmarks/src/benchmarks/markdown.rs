//! Markdown report generation
//!
//! This module provides functionality to generate human-readable markdown
//! reports from benchmark results. Reports include formatted tables,
//! summaries, and metadata.

use crate::benchmarks::result::BenchmarkResult;
use anyhow::Result;
use std::collections::HashSet;

/// Generates a markdown report from a collection of benchmark results
///
/// The generated report includes:
/// - Executive summary with timestamp and system information
/// - Results table with all metrics
/// - Individual benchmark details
///
/// # Arguments
///
/// * `results` - Collection of benchmark results to include in the report
///
/// # Returns
///
/// A `Result` containing the markdown report as a string
///
/// # Example
///
/// ```no_run
/// use marketplace_benchmarks::{BenchmarkResult, generate_markdown_report};
/// use std::collections::HashMap;
///
/// let mut metrics = HashMap::new();
/// metrics.insert("latency_p50".to_string(), 12.5);
///
/// let result = BenchmarkResult::new("api-gateway".to_string(), metrics);
/// let report = generate_markdown_report(&[result]).unwrap();
/// println!("{}", report);
/// ```
pub fn generate_markdown_report(results: &[BenchmarkResult]) -> Result<String> {
    let mut report = String::new();

    // Header
    report.push_str("# Marketplace Benchmark Report\n\n");

    // Summary section
    report.push_str("## Executive Summary\n\n");
    if let Some(first_result) = results.first() {
        report.push_str(&format!(
            "**Report Generated:** {}\n\n",
            first_result.timestamp.format("%Y-%m-%d %H:%M:%S UTC")
        ));
    }
    report.push_str(&format!("**Total Benchmarks:** {}\n\n", results.len()));

    // Add system information if available
    if let Some(first_result) = results.first() {
        if let Some(hostname) = first_result.get_metadata("hostname") {
            report.push_str(&format!("**Hostname:** {}\n", hostname));
        }
        if let Some(cpu_count) = first_result.get_metadata("cpu_count") {
            report.push_str(&format!("**CPU Count:** {}\n", cpu_count));
        }
        if let Some(os) = first_result.get_metadata("os") {
            report.push_str(&format!("**OS:** {}\n", os));
        }
        report.push('\n');
    }

    // Collect all unique metric keys
    let mut all_metric_keys: HashSet<String> = HashSet::new();
    for result in results {
        for key in result.metrics.keys() {
            all_metric_keys.insert(key.clone());
        }
    }
    let mut sorted_keys: Vec<String> = all_metric_keys.into_iter().collect();
    sorted_keys.sort();

    // Results table
    report.push_str("## Benchmark Results\n\n");
    report.push_str("| Target | ");
    for key in &sorted_keys {
        report.push_str(&format!("{} | ", key));
    }
    report.push_str("\n|--------|");
    for _ in &sorted_keys {
        report.push_str("--------|");
    }
    report.push('\n');

    for result in results {
        report.push_str(&format!("| {} | ", result.target_id));
        for key in &sorted_keys {
            if let Some(value) = result.get_metric(key) {
                report.push_str(&format!("{:.2} | ", value));
            } else {
                report.push_str("N/A | ");
            }
        }
        report.push('\n');
    }
    report.push('\n');

    // Detailed results
    report.push_str("## Detailed Results\n\n");
    for result in results {
        report.push_str(&format!("### {}\n\n", result.target_id));
        report.push_str(&format!(
            "**Timestamp:** {}\n\n",
            result.timestamp.format("%Y-%m-%d %H:%M:%S UTC")
        ));

        // Metrics
        report.push_str("**Metrics:**\n\n");
        let mut metric_keys: Vec<&String> = result.metrics.keys().collect();
        metric_keys.sort();
        for key in metric_keys {
            if let Some(value) = result.get_metric(key) {
                report.push_str(&format!("- {}: {:.2}\n", key, value));
            }
        }
        report.push('\n');

        // Metadata if present
        if !result.metadata.is_empty() {
            report.push_str("**Metadata:**\n\n");
            let mut metadata_keys: Vec<&String> = result.metadata.keys().collect();
            metadata_keys.sort();
            for key in metadata_keys {
                if let Some(value) = result.get_metadata(key) {
                    report.push_str(&format!("- {}: {}\n", key, value));
                }
            }
            report.push('\n');
        }
    }

    // Footer
    report.push_str("---\n\n");
    report.push_str("*Report generated by marketplace-benchmarks*\n");

    Ok(report)
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::collections::HashMap;

    #[test]
    fn test_generate_empty_report() {
        let report = generate_markdown_report(&[]).unwrap();
        assert!(report.contains("# Marketplace Benchmark Report"));
        assert!(report.contains("**Total Benchmarks:** 0"));
    }

    #[test]
    fn test_generate_single_result_report() {
        let mut metrics = HashMap::new();
        metrics.insert("latency_p50".to_string(), 12.5);
        metrics.insert("throughput".to_string(), 1000.0);

        let result = BenchmarkResult::new("test-target".to_string(), metrics);
        let report = generate_markdown_report(&[result]).unwrap();

        assert!(report.contains("# Marketplace Benchmark Report"));
        assert!(report.contains("**Total Benchmarks:** 1"));
        assert!(report.contains("test-target"));
        assert!(report.contains("latency_p50"));
        assert!(report.contains("throughput"));
        assert!(report.contains("12.50"));
        assert!(report.contains("1000.00"));
    }

    #[test]
    fn test_generate_multiple_results_report() {
        let mut metrics1 = HashMap::new();
        metrics1.insert("latency_p50".to_string(), 12.5);

        let mut metrics2 = HashMap::new();
        metrics2.insert("latency_p50".to_string(), 8.3);
        metrics2.insert("throughput".to_string(), 2000.0);

        let result1 = BenchmarkResult::new("target-1".to_string(), metrics1);
        let result2 = BenchmarkResult::new("target-2".to_string(), metrics2);

        let report = generate_markdown_report(&[result1, result2]).unwrap();

        assert!(report.contains("**Total Benchmarks:** 2"));
        assert!(report.contains("target-1"));
        assert!(report.contains("target-2"));
        assert!(report.contains("N/A")); // target-1 doesn't have throughput
    }

    #[test]
    fn test_report_with_metadata() {
        let mut metrics = HashMap::new();
        metrics.insert("latency_p50".to_string(), 12.5);

        let mut metadata = HashMap::new();
        metadata.insert("version".to_string(), "1.0.0".to_string());
        metadata.insert("hostname".to_string(), "test-host".to_string());

        let result = BenchmarkResult::with_metadata(
            "test-target".to_string(),
            metrics,
            metadata,
        );

        let report = generate_markdown_report(&[result]).unwrap();

        assert!(report.contains("**Hostname:** test-host"));
        assert!(report.contains("**Metadata:**"));
        assert!(report.contains("version: 1.0.0"));
    }
}
